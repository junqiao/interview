Three tables (one for storing all items, one for all tags, and one for the relation between the two), properly indexed, with foreign keys set running on a proper database, should work well and scale properly. 

	Table: Item
	Columns: ItemID, Title, Content

	Table: Tag
	Columns: TagID, Title

	Table: ItemTag
	Columns: ItemID, TagID

Algorithm
1. Topic modeling
	* each document consists of a mixture of topics and each topic consists of a collection of words. The goal of modeling is to uncover the latent topics of document.
	* LSA - Latent semantic analysis. 
		** step 1: generate document-term matrix for our document. Given m documents and n words in our vocabulary, we can construct an m*n matrix A in which each row represents a document and each column represents a word. In the simplest version of LSA, each entry can simply be a row count of the number of times the j-th word appeared in the i-th document. Because row count doesn't account for the significance of each word in document, it doesn't work well.
		** step 2: replace raw count with tf-idf score. 
		** step 3: perform dimensionality reduction on matrix A because A is sparse, noisy, and redundant. The dimensionlity reduction can be preformed using truncated SVD (singular value decomposition. select only the t largest singular values). A = U S V^T.

		** quick and efficient to use
		** drawback
			- lack of interpretable embeddings(the components may be arbitrarily positive/negative)
			- need for really large set of documents and vocabulary to get accurate results
			- less effieicnt representation

		** PCA (Principal Component Analysis) 

	* PLSA - Probabilistic latent semantic analysis
		** find a probabilistic model with latent topics that can generate the data we observe in our document-term matrix. 
		** given a document d, topic z is present in that document with probability P(z|d). given a topic z, word w is drawn from z with probability P(w|z). we can get the joint probability p(d, w) = p(d) sum(p(z|d)p(w|z))

	* LDA - Latent Dirichlet Allocation
		http://cseweb.ucsd.edu/~dhu/docs/research_exam09.pdf
		** LDA is a Bayesian version of PLSA (In PLSA, we assume the probability is a fixed point but in LDA, probability is uncertain. we sample probability by dirichlet distribution).
		** Step 1: draw a topic weight vector (modeled by Dirichlet random variable) that determined which topics are most likely to appear in a document. For each word that is to appear in the document, we choose a single topic from the topic weight vector. 
		** Step 2: to actually generate the word, we then draw from the probability distribution conditioned on the chosen topic. From this procedure, we see that each word in a document is generated by a different, randomly chosen topic. The available choice of topics for each document are, in turn, drawn from a smooth distribution over the space of all possible topics. 
		** Since the generatibe process imagines each word to be generated by a different topic, the LDA model allows documents to exhibit multiple topics to different degrees. This overcomes the limitations of the mixture of unigrams model which only allows a single topic per document. 
		** Gibbs Sampling - approximate the distribution of Dirichlet.
		** use 'bag-of-words' concept
		** why Dirichlet? it is in the exponential family, has finite dimensional sufficient statistics, and is a conjugate prior to multinomial distribution. These propertieswill make the inference and parameter estimation algorithms for LDA simpler.

	
	* LDA in Deep Learning - lda2vec - an extension of word2vec and LDA that jointly learns word, document, and topic vectors

	* bag of words model - used to train a classifier by occurrence of each word
		** supposed we have two text documents, one is 'John likes to watch movies. Mary likes movies too' and another one is 'Mary also likes to watch football games'. we have a list of all words ["John", "likes", "to", "watch", "movies", "also", "football", "games", "Mary", "too"] and can represent two string using array [1, 2, 1, 1, 2, 0, 0, 0, 1, 1] and [1, 1, 1, 1, 0, 1, 1, 1, 0, 0]. we can use TF-IDF algorithm to weigh every word. 
	
	* n-gram model - a contiguous sequence of n items from a given sample of text or speech
		** For example, if n = 2, we can get ["John likes", "likes to", "to watch", "watch movies", "Mary likes", "likes movies", "movies too",] from "John likes to watch movies. Mary likes movies too".
	
	* binomial distribution - if n = 1, Bernoulli distribution.
		** binomial coefficient
	* Multinomial Distribution - instead of having two options like binomal distribution. it has multiple options. 
	* Gamma function - T(x+1) = xT(x)
	* Beta distribution
	* Conjugate prior distribution
	* Dirichlet distribution


TF-IDF: 
	* TF (term frequency): the raw count of a term in a document. Suppose cat occur 4 times in document m.txt which has 1000 words. TF = 4 / 1000
	* IDF (inverse document frequency): measure of how much information the word procides. Suppose we have 10000 documents and cat occurs in 100 documents. IDF = lg(10000/(100 + 1)). The reason why 100 + 1 is to prevent conner case that word cat occur 0 time in all document. 
	* final result is TF * IDF
	* drawback: no way to measure the importance of a word in a paragraph.

2. Automatic Tag Recommendation Algorithms forSocial Recommender Systems
https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tagging-1.pdf
